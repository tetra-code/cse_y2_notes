FLP impossibility - in an async network, consensus can't reach if at least one node fails

Async communications


Byzantine general problem:
*we assume only one traitor (malicious) here. 
In real life, not sure exactly how many

PBFT - *Byzantine fault-tolerant* consensus: For N nodes, f faulty ones, N must be of at least size N>=3f+1 in order to reach tolerant consensus
If N is not big enough, maybe able to detect partial malicious nodes but definitely not all of them

Paxos, Raft - *Crash fault-tolerant* consensus with at least N>=2f+1. Here we won't get any malicious messages. So having a  
f + f + 1

In theory, we assume no malicious nodes so no Byzantine scenario. In practice, byzantine scenario more in real life.

State-machine: some logical system that can be in multiple states. A set of transactions defines the transition from one state to another. Normally we assume finite states. (if we define with RTC it can be considered infinite states)

State

## Paxos protocol:
By Lamport. Homogenous system?

Three roles
- Proposer: chooses a value (or receive from client) and sent itt  oa a set of 'acceptors' to collect votes (kind of a leader but not really as it depends on who received the client message)
- Acceptor: Vote to accept or reject the vote
- Learner:

Paxos is actually hard to implement correctly, not many handling of edge cases

In paxos anyone can become a proposer (whoever receives the message from client). THus not guaranteed to be proposer for next client message

In case of two possible proposers (each receive message from different clients at the same time), the node with the higher bottle number will executed first and then the other. 
## Raft consensus algorithm 
Leader based asymmetic model. This is able to choose a new leader once the crash is detected. REmember that this is not byzantine fault-tolerant, only crash fault-tolerant.


Three roles:
- Leader
- Follower
- Candidate

The process starts with the follower 9not leader). If none, becomes for candidate. The majority becomes the leader. The rest can stay in candidate or hears from leader and become follower. Unl

(LEader constantly receives heartbeat from others, and send msg to only the lagging behind node)

IF two concurrent requests, the same leader will get both requests.

If there are multiple candidates for votes and none has the majority of nodes, if some of the vote msgs are lost and no majority vote time for vote againt after timeout (time out is also random for each process). Use randomness and timeouts 


The term value of one value can be different from . Every has current term value (not global view). This taerm value is exchanged in every RPC, 

Guaranteed Raft selects at most one leader at each term

ONce leader is selected, it does log replication. A log entry is committed once the leader that creates the entyr has replicated it on a majority of the servers. 

Commit index is at 7 in the slides. The oens that don't have commit nidex 7 must have been partitioned before index 7 made. The ones with missing indices need to receied first. *anything not committed is lost

Leader appends 9 and then crash wihtout replicate. In taht case only middle can be leader since it has the hightest latest. 
*different colors repreent each term (commites made during different leadership)

FOr the ones without all the commits, those nodes will  get updated once back into the network AND hear from the leader. Before being elected asleader, we must update the term

Raft can 
## Distrubted database
For sync, if another operation in the queue, need to wait until the current one is done for ALL the followers.
For asyn if leader goes down, no way of knowing which nodes successed and which ones failed. In addition all nodes not guaranteed to have same consistent state since each process and finish time can be different and can have different states at a given point.

Hybrid async and sync: 
